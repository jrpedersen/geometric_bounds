{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "ynf1MnfKLj2r"
      },
      "id": "ynf1MnfKLj2r"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning -qq"
      ],
      "metadata": {
        "id": "PN4JI_T2IsEQ",
        "outputId": "d48dd5a9-bc35-4639-e24a-d9c66084f865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PN4JI_T2IsEQ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m826.4/826.4 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a2685c55-5814-47e6-ad46-ba328d715b64",
      "metadata": {
        "id": "a2685c55-5814-47e6-ad46-ba328d715b64"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import itertools\n",
        "from typing import Optional\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "\n",
        "#import git\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, linalg\n",
        "from torch.nn import functional as F\n",
        "from torchmetrics.functional import accuracy\n",
        "\n",
        "\n",
        "import torchmetrics\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Note - you must have torchvision installed for this example\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "#from src.models.mlp import SkipMLP, SimpleMLP\n",
        "#from src.data.mnist import MNISTDataModule\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# geometric bounds"
      ],
      "metadata": {
        "id": "0UbrMEHbLm4y"
      },
      "id": "0UbrMEHbLm4y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baced7ed-1975-4aac-87eb-1fe578d9de70",
      "metadata": {
        "id": "baced7ed-1975-4aac-87eb-1fe578d9de70"
      },
      "outputs": [],
      "source": [
        "#from collections import OrderedDict\n",
        "from functools import partial\n",
        "#import torch\n",
        "#from torch import nn, optim, linalg\n",
        "from functorch import jacrev, vmap\n",
        "\n",
        "\n",
        "def calc_bound(w, h, jh):\n",
        "    return (\n",
        "        (1 + linalg.vector_norm(h, dim=1)**2) /\n",
        "        (linalg.matrix_norm(w, ord=2)**2 *\n",
        "        linalg.matrix_norm(jh, ord='fro')**2)\n",
        "    )\n",
        "\n",
        "def calc_tight_bound(w, h, jh):\n",
        "    return (\n",
        "        (1 + linalg.vector_norm(h, dim=1)**2) /\n",
        "        (linalg.matrix_norm(torch.bmm(w, jh), ord='fro')**2)\n",
        "    )\n",
        "\n",
        "def mlp_bound(weight_layer, partial_net, x, bound_f = calc_tight_bound):\n",
        "    h_0 = x.view(x.shape[0],-1)\n",
        "    if len(partial_net) > 0:\n",
        "        h_n = partial_net(h_0)\n",
        "        jh_n = vmap(jacrev(partial_net))(h_0)\n",
        "    else:\n",
        "        h_n = h_0\n",
        "        jh_n = torch.eye(h_0.shape[1]).unsqueeze(0).repeat(x.shape[0],1,1)\n",
        "\n",
        "    w = weight_layer.weight.repeat(x.shape[0],1,1)\n",
        "    return bound_f(w, h_n, jh_n), (w,h_n,jh_n)\n",
        "\n",
        "def skip_mlp_bound(weight_layer, partial_net, x, bound_f = calc_tight_bound):\n",
        "    negative_slope = weight_layer.activation.negative_slope\n",
        "\n",
        "    h_0 = x.view(x.shape[0],-1)\n",
        "    h_n = partial_net(h_0)\n",
        "    jh_n = vmap(jacrev(partial_net))(h_0)\n",
        "\n",
        "    w_base = weight_layer.fc.weight.repeat(x.shape[0],1,1)\n",
        "    w_skip =torch.eye(*w_base.shape[1:]).unsqueeze(0).repeat(x.shape[0],1,1)\n",
        "    w_skip = w_skip * (1 + (torch.gt(-weight_layer.fc(h_n), 0) * (negative_slope**(-1) - 1 ))).unsqueeze(1)\n",
        "    w = w_base + w_skip\n",
        "    return bound_f(w, h_n, jh_n), (w,h_n,jh_n)\n",
        "\n",
        "def get_bounds(model, x):\n",
        "    modules_list = list(model.layers._modules.items())\n",
        "    bound_types = {\n",
        "        'fc': mlp_bound,\n",
        "        'sk': skip_mlp_bound\n",
        "    }\n",
        "    layer_wise_bound = []\n",
        "    w= []\n",
        "    h= []\n",
        "    h_prime = []\n",
        "    for i in range(len(modules_list)):\n",
        "        name, layer = modules_list[i]\n",
        "        if name[:2] in bound_types.keys():\n",
        "            bound, parts_tuple = bound_types[name[:2]](\n",
        "                layer,\n",
        "                nn.Sequential(OrderedDict(modules_list[:i])),\n",
        "                x\n",
        "            )\n",
        "            layer_wise_bound.append(bound)\n",
        "            w.append(parts_tuple[0])\n",
        "            h.append(parts_tuple[1])\n",
        "            h_prime.append(parts_tuple[2])\n",
        "    return torch.stack(layer_wise_bound).T.detach()#, torch.stack(w).T.detach(), torch.stack(h).T.detach(), torch.stack(h_prime).T.detach()\n",
        "\n",
        "def compose_funcs(f,g): return lambda x : g(f(x))\n",
        "\n",
        "def norm_grad_params(model):\n",
        "    return torch.tensor([(param.grad**2).sum() for param in model.parameters()])\n",
        "\n",
        "def norm_grad_x(model, loss_fn, x, labels):\n",
        "    grad_x = jacrev(compose_funcs(model, partial(loss_fn,target=labels)))(x)\n",
        "    return (grad_x**2).sum().detach()\n",
        "\n",
        "def norm_grads(model, loss_fn, optimizer, x, labels):\n",
        "    #optimizer.zero_grad()\n",
        "    inputs = x.clone()\n",
        "    inputs.requires_grad_(True)\n",
        "    inputs.retain_grad()\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "\n",
        "    norm_grad_wrt_input = (inputs.grad**2).sum()\n",
        "    norm_grad_wrt_params = torch.tensor([(param.grad**2).sum() for param in model.parameters()])\n",
        "    for param in model.parameters():\n",
        "        param.grad = None\n",
        "\n",
        "    return (\n",
        "        norm_grad_wrt_input,\n",
        "        norm_grad_wrt_params\n",
        "    )\n",
        "\n",
        "def norm_gradients(model, loss_fn, inputs, labels):\n",
        "    norm_gradients_batch=[]\n",
        "    for _img, _label in zip(inputs, labels):\n",
        "        img = _img.unsqueeze(0)\n",
        "        label= _label.unsqueeze(0)\n",
        "        norm_gradients_batch.append(norm_grads(model, loss_fn,None, img, label))\n",
        "    return [torch.stack(gradients) for gradients in zip(*norm_gradients_batch)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model definition"
      ],
      "metadata": {
        "id": "fpZX-o7UL5zd"
      },
      "id": "fpZX-o7UL5zd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## block definitions"
      ],
      "metadata": {
        "id": "RhPaWmSe9E5p"
      },
      "id": "RhPaWmSe9E5p"
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMlpBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features, negative_slope):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=negative_slope)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.activation(self.fc(x))\n",
        "\n",
        "class SkipMlpBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features, negative_slope):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=negative_slope)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x + self.activation(self.fc(x))"
      ],
      "metadata": {
        "id": "82vwGnDM8jYQ"
      },
      "id": "82vwGnDM8jYQ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## simple model architecture"
      ],
      "metadata": {
        "id": "556KK1tl9H3D"
      },
      "id": "556KK1tl9H3D"
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        next_layer_input = config[\"input_size\"]#784\n",
        "        #layers = []\n",
        "        hidden_layers = config['hl_depth'] * [config['hl_width']]\n",
        "        layers = dict()\n",
        "        BlockType = SkipMlpBlock if config[\"use_skip\"] else SimpleMlpBlock\n",
        "        for _i, hidden_layer in enumerate(hidden_layers):\n",
        "            if next_layer_input == hidden_layer:\n",
        "                layers.update({'skip_fc'+str(_i): BlockType(in_features=next_layer_input, out_features=hidden_layer, negative_slope=config['negative_slope'])})\n",
        "            else:\n",
        "                layers.update({'fc'+ str(_i): nn.Linear(in_features=next_layer_input, out_features=hidden_layer)})\n",
        "                layers.update({'af'+ str(_i): nn.LeakyReLU(negative_slope=config['negative_slope'])})\n",
        "            # Update input size\n",
        "            next_layer_input = hidden_layer\n",
        "\n",
        "        layers.update({'fc'+ str(len(hidden_layers)): nn.Linear(in_features=next_layer_input, out_features=config[\"out_features\"])})\n",
        "        #layers.update({'af'+ str(len(hidden_layers)): nn.LeakyReLU(negative_slope=config['negative_slope'])})\n",
        "        self.layers = nn.Sequential(layers)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, _, _, _ = x.size()\n",
        "        x_resized = x.view(batch_size, -1)\n",
        "        return self.layers(x_resized)"
      ],
      "metadata": {
        "id": "MHw-bb9h8k-c"
      },
      "id": "MHw-bb9h8k-c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training run script"
      ],
      "metadata": {
        "id": "bQ_q74xe9KxC"
      },
      "id": "bQ_q74xe9KxC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cafbf368-6629-440c-990c-3dac4fd0cc8a",
      "metadata": {
        "id": "cafbf368-6629-440c-990c-3dac4fd0cc8a"
      },
      "outputs": [],
      "source": [
        "#from collections import OrderedDict\n",
        "#import pdb\n",
        "#import torch\n",
        "#import pytorch_lightning as pl\n",
        "#import src.func_geometric_bounds as gb\n",
        "\n",
        "class RunBase(pl.LightningModule):\n",
        "    def __init__(self, model, config):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        data, target = batch\n",
        "\n",
        "        # Bound statistics\n",
        "        if batch_idx %16==0:\n",
        "            #bounds, w, h, jh = gb.get_bounds(self, data)\n",
        "            bounds = gb.get_bounds(self, data)\n",
        "            gradients_data, gradients_params = gb.norm_gradients(self, nn.CrossEntropyLoss(label_smoothing=0.1), data, target)\n",
        "            self.log('bounds/Gradients_x', gradients_data.sum())\n",
        "            self.log('bounds/Bound', bounds.sum())\n",
        "            self.log('bounds/Bound div Gradients_x', (bounds.sum(dim=1)/gradients_data).mean())\n",
        "            self.log('bounds/Gradients_x div Bound', (gradients_data/bounds.sum(dim=1)).mean())\n",
        "            self.log('bounds/Gradients_x times bound',(\n",
        "                gradients_data* bounds.sum(dim=1)\n",
        "            ).sum(), on_step=True)\n",
        "            self.log('bounds/Gradients parameters',gradients_params.sum(), on_step=True)\n",
        "\n",
        "            #self.log('bounds/w dot jh', linalg.matrix_norm(torch.bmm(w, jh), ord='fro').mean())\n",
        "            #self.log('bounds/h',linalg.vector_norm(h, dim=1).mean())\n",
        "\n",
        "\n",
        "        preds = self(data)\n",
        "        loss = F.cross_entropy(preds, target, label_smoothing=0.1)\n",
        "        # Logging to TensorBoard by default\n",
        "        self.log('train/acc', accuracy(preds, target), on_step=True, on_epoch=True)\n",
        "        self.log(\"train/loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, valid_batch, batch_idx):\n",
        "        data, target = valid_batch\n",
        "        preds = self(data)\n",
        "        _, max_pred = torch.max(preds, 1)\n",
        "        loss = F.cross_entropy(preds, target, label_smoothing=0.1)\n",
        "        self.log(\"validation/loss\", loss)\n",
        "        self.log('validation/acc', accuracy(preds, target), on_step=True, on_epoch=True)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dataset definition"
      ],
      "metadata": {
        "id": "3BwrXGjWNOWP"
      },
      "id": "3BwrXGjWNOWP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aa8a15f-66a2-4f1d-810f-9fddd817da34",
      "metadata": {
        "id": "9aa8a15f-66a2-4f1d-810f-9fddd817da34"
      },
      "outputs": [],
      "source": [
        "# Note - you must have torchvision installed for this example\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir: str = \"./\", batch_size = 32):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # download\n",
        "        MNIST(self.data_dir, train=True, download=True)\n",
        "        MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None):\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "        if stage == \"predict\" or stage is None:\n",
        "            self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return DataLoader(self.mnist_predict, batch_size=self.batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run"
      ],
      "metadata": {
        "id": "c8ebba6d-6fb4-4dee-a396-6e9e15a7971d"
      },
      "id": "c8ebba6d-6fb4-4dee-a396-6e9e15a7971d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50725235-fc93-43b9-8606-66c970fcefd6",
      "metadata": {
        "id": "50725235-fc93-43b9-8606-66c970fcefd6"
      },
      "outputs": [],
      "source": [
        "def main(config):\n",
        "    mnist = MNISTDataModule(data_dir=\"./data\", batch_size=config['batch_size'])\n",
        "    model = config['model_type'](config)\n",
        "    model_name = re.findall(r\"[\\w]+\", str(type(model)))[-1]\n",
        "\n",
        "    logger = TensorBoardLogger(\n",
        "        'lightning_logs/',\n",
        "        name=model_name,\n",
        "        version=\"depth\"+str(config['hl_depth'])\n",
        "    )\n",
        "    logger.log_hyperparams(config)\n",
        "\n",
        "    trainer = pl.Trainer(max_epochs=config['max_epochs'],\n",
        "                         num_processes=1,\n",
        "                         #accelerator='gpu',\n",
        "                         #devices=1,\n",
        "                         logger=logger,\n",
        "                         deterministic=True)\n",
        "    trainer.fit(model, datamodule=mnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "974dec8a-7342-4a50-90b0-ac79e70b4b36",
      "metadata": {
        "id": "974dec8a-7342-4a50-90b0-ac79e70b4b36"
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(1234)\n",
        "# Git current git commit:\n",
        "repo = git.Repo(search_parent_directories=True)\n",
        "sha = repo.head.object.hexsha\n",
        "config = {\n",
        "    # git revision\n",
        "    'sha': sha,\n",
        "    # dataset\n",
        "    'batch_size': 32,\n",
        "    # model config\n",
        "    'model_type': SimpleMLP,\n",
        "    #'num_parameters': (28**2 + 10) * w + (h-1)*w**2 # for single P 28**2 * 10\n",
        "    #'hl_depth': 2,\n",
        "    #'hl_width': 40,\n",
        "    'negative_slope': 0.01,\n",
        "    # training\n",
        "    'max_epochs': 20,\n",
        "}\n",
        "# w * ((28*2 + 10) + (h-1)*w) aaprox h*w^2\n",
        "# TODO: Find way to split depth versus wide.\n",
        "# (h,w) = [(1,8), (4,4), (16,2), (64,1), () ]\n",
        "# TODO: Change loop to set width and weight dependent on num parameters\n",
        "# TODO: Perhaps around 10 layers of 40x40 size ish. And 1,2,4 to that.\n",
        "# (2**4 * 10**4)\n",
        "# OLD\n",
        "num_params = lambda w, h : (h-1)*w**2 + (28**2 + 10) * w\n",
        "base_width = 8 * 40 //2\n",
        "base_depth = 1\n",
        "\n",
        "for n in range(1):\n",
        "    config['hl_width'] = base_width // (2**n)\n",
        "    config['hl_depth'] = base_depth * 2**(2*n) + int(\n",
        "        2**(-0.5) * 2**(2**0.75*(n-1)) * (n>0) * (28**2 + 10)/config['hl_width'] #* (1-2**(-n))\n",
        "    )\n",
        "    #print(config['hl_width'])\n",
        "    #print(config['hl_depth'])\n",
        "    #print(num_params(config['hl_width'], config['hl_depth']))\n",
        "    print(config)\n",
        "    import ipdb; ipdb.set_trace()\n",
        "    main(config)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RhPaWmSe9E5p",
        "3BwrXGjWNOWP"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}