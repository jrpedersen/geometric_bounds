{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2685c55-5814-47e6-ad46-ba328d715b64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _C: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MNIST\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SkipMLP, SimpleMLP\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmnist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MNISTDataModule\n",
      "File \u001b[1;32m~\\geometric_bounds\\src\\models\\mlp.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunc_geometric_bounds\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgb\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRunBase\u001b[39;00m(pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[1;32m~\\geometric_bounds\\src\\func_geometric_bounds.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, optim, linalg\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jacrev, vmap\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_bound\u001b[39m(w, h, jh):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\functorch\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _C\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Monkey patch PyTorch. This is a hack, we should try to upstream\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# these pieces.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monkey_patching \u001b[38;5;28;01mas\u001b[39;00m _monkey_patching\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _C: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import itertools\n",
    "from typing import Optional\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "import git\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, linalg\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Note - you must have torchvision installed for this example\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "#from src.models.mlp import SkipMLP, SimpleMLP\n",
    "#from src.data.mnist import MNISTDataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baced7ed-1975-4aac-87eb-1fe578d9de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch import nn, optim, linalg\n",
    "from functorch import jacrev, vmap\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "def calc_bound(w, h, jh):\n",
    "    return (\n",
    "        (1 + linalg.vector_norm(h, dim=1)**2) /\n",
    "        (linalg.matrix_norm(w, ord=2)**2 *\n",
    "        linalg.matrix_norm(jh, ord='fro')**2)\n",
    "    )\n",
    "\n",
    "def calc_tight_bound(w, h, jh):\n",
    "    return (\n",
    "        (1 + linalg.vector_norm(h, dim=1)**2) /\n",
    "        (linalg.matrix_norm(torch.bmm(w, jh), ord='fro')**2)\n",
    "    )\n",
    "\n",
    "def mlp_bound(weight_layer, partial_net, x, bound_f = calc_tight_bound):\n",
    "    h_0 = x.view(x.shape[0],-1)\n",
    "    if len(partial_net) > 0:\n",
    "        h_n = partial_net(h_0)\n",
    "        jh_n = vmap(jacrev(partial_net))(h_0)\n",
    "    else:\n",
    "        h_n = h_0\n",
    "        jh_n = torch.eye(h_0.shape[1]).unsqueeze(0).repeat(x.shape[0],1,1)\n",
    "\n",
    "    w = weight_layer.weight.repeat(x.shape[0],1,1)\n",
    "    return bound_f(w, h_n, jh_n), (w,h_n,jh_n)\n",
    "\n",
    "def skip_mlp_bound(weight_layer, partial_net, x, bound_f = calc_tight_bound):\n",
    "    negative_slope = weight_layer.activation.negative_slope\n",
    "\n",
    "    h_0 = x.view(x.shape[0],-1)\n",
    "    h_n = partial_net(h_0)\n",
    "    jh_n = vmap(jacrev(partial_net))(h_0)\n",
    "\n",
    "    w_base = weight_layer.fc.weight.repeat(x.shape[0],1,1)\n",
    "    w_skip =torch.eye(*w_base.shape[1:]).unsqueeze(0).repeat(x.shape[0],1,1)\n",
    "    w_skip = w_skip * (1 + (torch.gt(-weight_layer.fc(h_n), 0) * (negative_slope**(-1) - 1 ))).unsqueeze(1)\n",
    "    w = w_base + w_skip\n",
    "    return bound_f(w, h_n, jh_n), (w,h_n,jh_n)\n",
    "\n",
    "def get_bounds(model, x):\n",
    "    modules_list = list(model.layers._modules.items())\n",
    "    bound_types = {\n",
    "        'fc': mlp_bound,\n",
    "        'sk': skip_mlp_bound\n",
    "    }\n",
    "    layer_wise_bound = []\n",
    "    w= []\n",
    "    h= []\n",
    "    h_prime = []\n",
    "    for i in range(len(modules_list)):\n",
    "        name, layer = modules_list[i]\n",
    "        if name[:2] in bound_types.keys():\n",
    "            bound, parts_tuple = bound_types[name[:2]](\n",
    "                layer,\n",
    "                nn.Sequential(OrderedDict(modules_list[:i])),\n",
    "                x\n",
    "            )\n",
    "            layer_wise_bound.append(bound)\n",
    "            w.append(parts_tuple[0])\n",
    "            h.append(parts_tuple[1])\n",
    "            h_prime.append(parts_tuple[2])\n",
    "    return torch.stack(layer_wise_bound).T.detach()#, torch.stack(w).T.detach(), torch.stack(h).T.detach(), torch.stack(h_prime).T.detach()\n",
    "\n",
    "def compose_fns(f,g): return lambda x : g(f(x))\n",
    "\n",
    "def norm_grad_params(model):\n",
    "    return torch.tensor([(param.grad**2).sum() for param in model.parameters()])\n",
    "\n",
    "def norm_grad_x(model, loss_fn, x, labels):\n",
    "    grad_x = jacrev(compose_fns(model, partial(loss_fn,target=labels)))(x)\n",
    "    return (grad_x**2).sum().detach()\n",
    "\n",
    "def norm_grads(model, loss_fn, optimizer, x, labels):\n",
    "    #optimizer.zero_grad()\n",
    "    inputs = x.clone()\n",
    "    inputs.requires_grad_(True)\n",
    "    inputs.retain_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    loss.backward()\n",
    "\n",
    "    norm_grad_wrt_input = (inputs.grad**2).sum()\n",
    "    norm_grad_wrt_params = torch.tensor([(param.grad**2).sum() for param in model.parameters()])\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "\n",
    "    return (norm_grad_wrt_input,\n",
    "        norm_grad_wrt_params\n",
    "    )\n",
    "\n",
    "def norm_gradients(model, loss_fn, inputs, labels):\n",
    "    norm_gradients_batch=[]\n",
    "    for _img, _label in zip(inputs, labels):\n",
    "        img = _img.unsqueeze(0)\n",
    "        label= _label.unsqueeze(0)\n",
    "        norm_gradients_batch.append(norm_grads(model, loss_fn,None, img, label))\n",
    "    return [torch.stack(gradients) for gradients in zip(*norm_gradients_batch)]\n",
    "\n",
    "\n",
    "class BoundSampler(pl.Callback):\n",
    "    \"\"\"Logs to tensorboard.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples_per_epoch) -> None:\n",
    "        super().__init__()\n",
    "        self.state = 0\n",
    "        self.num_samples = num_samples_per_epoch\n",
    "    #def on_train_batch_end(self, *args, **kwargs):#, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:\n",
    "    def on_train_batch_end(self, *args, **kwargs):\n",
    "        if self.state < self.num_samples:\n",
    "            loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "            trainer, model, _, batch, batch_idx = args\n",
    "            inputs, labels = batch\n",
    "\n",
    "            bounds = get_bounds(model, inputs)\n",
    "            norm_gradients_batch=norm_gradients(model, loss_fn, inputs,labels)#img, label)\n",
    "            model.log('Bound',bounds.sum())\n",
    "            self.state =+ 1\n",
    "    def on_train_epoch_end(self, *args, **kwargs):\n",
    "        self.state = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafbf368-6629-440c-990c-3dac4fd0cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import pdb\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchmetrics.functional import accuracy\n",
    "import pytorch_lightning as pl\n",
    "import src.func_geometric_bounds as gb\n",
    "\n",
    "class RunBase(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, target = batch\n",
    "\n",
    "        # Bound statistics\n",
    "        if batch_idx %16==0:\n",
    "            #bounds, w, h, jh = gb.get_bounds(self, data)\n",
    "            bounds = gb.get_bounds(self, data)\n",
    "            gradients_data, gradients_params = gb.norm_gradients(self, nn.CrossEntropyLoss(label_smoothing=0.1), data, target)\n",
    "            self.log('bounds/Gradients_x', gradients_data.sum())\n",
    "            self.log('bounds/Bound', bounds.sum())\n",
    "            self.log('bounds/Bound div Gradients_x', (bounds.sum(dim=1)/gradients_data).mean())\n",
    "            self.log('bounds/Gradients_x div Bound', (gradients_data/bounds.sum(dim=1)).mean())\n",
    "            self.log('bounds/Gradients_x times bound',(\n",
    "                gradients_data* bounds.sum(dim=1)\n",
    "            ).sum(), on_step=True)\n",
    "            self.log('bounds/Gradients parameters',gradients_params.sum(), on_step=True)\n",
    "\n",
    "            #self.log('bounds/w dot jh', linalg.matrix_norm(torch.bmm(w, jh), ord='fro').mean())\n",
    "            #self.log('bounds/h',linalg.vector_norm(h, dim=1).mean())\n",
    "\n",
    "\n",
    "        preds = self(data)\n",
    "        loss = F.cross_entropy(preds, target, label_smoothing=0.1)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train/acc', accuracy(preds, target), on_step=True, on_epoch=True)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, valid_batch, batch_idx):\n",
    "        data, target = valid_batch\n",
    "        preds = self(data)\n",
    "        _, max_pred = torch.max(preds, 1)\n",
    "        loss = F.cross_entropy(preds, target, label_smoothing=0.1)\n",
    "        self.log(\"validation/loss\", loss)\n",
    "        self.log('validation/acc', accuracy(preds, target), on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def create_hidden_layers(model_type, hidden_layers):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MLP(RunBase):\n",
    "    def __init__(self, config):\n",
    "        super(RunBase, self).__init__()\n",
    "        next_layer_input = 784\n",
    "        #layers = []\n",
    "        hidden_layers = config['hl_depth'] * [config['hl_width']]\n",
    "        layers = OrderedDict()\n",
    "\n",
    "        create_hidden_layers(config['model_type'], hidden_layers)\n",
    "\n",
    "        layers.update({'fc'+ str(len(hidden_layers)): nn.Linear(in_features=next_layer_input, out_features=10)})\n",
    "        #layers.update({'af'+ str(len(hidden_layers)): nn.LeakyReLU(negative_slope=config['negative_slope'])})\n",
    "        self.layers = nn.Sequential(layers)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _, _ = x.size()\n",
    "        x_resized = x.view(batch_size, -1)\n",
    "        return self.layers(x_resized)\n",
    "\n",
    "\n",
    "class SimpleMLP(RunBase):\n",
    "    def __init__(self, config):\n",
    "        super(RunBase, self).__init__()\n",
    "        next_layer_input = 784\n",
    "        #layers = []\n",
    "        hidden_layers = config['hl_depth'] * [config['hl_width']]\n",
    "        layers = OrderedDict()\n",
    "        for _i, hidden_layer in enumerate(hidden_layers):\n",
    "            # Create layer\n",
    "            layers.update({'fc'+ str(_i): nn.Linear(in_features=next_layer_input, out_features=hidden_layer)})\n",
    "            layers.update({'af'+ str(_i): nn.LeakyReLU(negative_slope=config['negative_slope'])})\n",
    "            # Update input size\n",
    "            next_layer_input = hidden_layer\n",
    "        layers.update({'fc'+ str(len(hidden_layers)): nn.Linear(in_features=next_layer_input, out_features=10)})\n",
    "        #layers.update({'af'+ str(len(hidden_layers)): nn.LeakyReLU(negative_slope=config['negative_slope'])})\n",
    "        self.layers = nn.Sequential(layers)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _, _ = x.size()\n",
    "        x_resized = x.view(batch_size, -1)\n",
    "        return self.layers(x_resized)\n",
    "\n",
    "\n",
    "class SkipMlpBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, negative_slope):\n",
    "        super(SkipMlpBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=in_features, out_features=out_features)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=negative_slope)\n",
    "    def forward(self, x):\n",
    "        return x + self.activation(self.fc(x))\n",
    "\n",
    "\n",
    "class SkipMLP(RunBase):\n",
    "    def __init__(self, config):\n",
    "        super(RunBase, self).__init__()\n",
    "        next_layer_input = 784\n",
    "        #layers = []\n",
    "        hidden_layers = config['hl_depth'] * [config['hl_width']]\n",
    "        layers = OrderedDict()\n",
    "        for _i, hidden_layer in enumerate(hidden_layers):\n",
    "            if next_layer_input == hidden_layer:\n",
    "                layers.update({'skip_fc'+str(_i): SkipMlpBlock(in_features=next_layer_input, out_features=hidden_layer, negative_slope=config['negative_slope'])})\n",
    "            else:\n",
    "                layers.update({'fc'+ str(_i): nn.Linear(in_features=next_layer_input, out_features=hidden_layer)})\n",
    "                layers.update({'af'+ str(_i): nn.LeakyReLU(negative_slope=config['negative_slope'])})\n",
    "            # Update input size\n",
    "            next_layer_input = hidden_layer\n",
    "        layers.update({'fc'+ str(len(hidden_layers)): nn.Linear(in_features=next_layer_input, out_features=10)})\n",
    "        #layers.update({'af'+ str(len(hidden_layers)): nn.LeakyReLU(negative_slope=config['negative_slope'])})\n",
    "        self.layers = nn.Sequential(layers)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _, _ = x.size()\n",
    "        x_resized = x.view(batch_size, -1)\n",
    "        return self.layers(x_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8a15f-66a2-4f1d-810f-9fddd817da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from typing import Optional\n",
    "# Note - you must have torchvision installed for this example\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\", batch_size = 32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "        if stage == \"predict\" or stage is None:\n",
    "            self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebba6d-6fb4-4dee-a396-6e9e15a7971d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50725235-fc93-43b9-8606-66c970fcefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    mnist = MNISTDataModule(data_dir=\"./data\", batch_size=config['batch_size'])\n",
    "    model = config['model_type'](config)\n",
    "    model_name = re.findall(r\"[\\w]+\", str(type(model)))[-1]\n",
    "\n",
    "    logger = TensorBoardLogger(\n",
    "        'lightning_logs/',\n",
    "        name=model_name,\n",
    "        version=\"depth\"+str(config['hl_depth'])\n",
    "    )\n",
    "    logger.log_hyperparams(config)\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=config['max_epochs'],\n",
    "                         num_processes=1,\n",
    "                         #accelerator='gpu',\n",
    "                         #devices=1,\n",
    "                         logger=logger,\n",
    "                         deterministic=True)\n",
    "    trainer.fit(model, datamodule=mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974dec8a-7342-4a50-90b0-ac79e70b4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(1234)\n",
    "# Git current git commit:\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "sha = repo.head.object.hexsha\n",
    "config = {\n",
    "    # git revision\n",
    "    'sha': sha,\n",
    "    # dataset\n",
    "    'batch_size': 32,\n",
    "    # model config\n",
    "    'model_type': SimpleMLP,\n",
    "    #'num_parameters': (28**2 + 10) * w + (h-1)*w**2 # for single P 28**2 * 10\n",
    "    #'hl_depth': 2,\n",
    "    #'hl_width': 40,\n",
    "    'negative_slope': 0.01,\n",
    "    # training\n",
    "    'max_epochs': 20,\n",
    "}\n",
    "# w * ((28*2 + 10) + (h-1)*w) aaprox h*w^2\n",
    "# TODO: Find way to split depth versus wide.\n",
    "# (h,w) = [(1,8), (4,4), (16,2), (64,1), () ]\n",
    "# TODO: Change loop to set width and weight dependent on num parameters\n",
    "# TODO: Perhaps around 10 layers of 40x40 size ish. And 1,2,4 to that.\n",
    "# (2**4 * 10**4)\n",
    "# OLD\n",
    "num_params = lambda w, h : (h-1)*w**2 + (28**2 + 10) * w\n",
    "base_width = 8 * 40 //2\n",
    "base_depth = 1\n",
    "\n",
    "for n in range(1):\n",
    "    config['hl_width'] = base_width // (2**n)\n",
    "    config['hl_depth'] = base_depth * 2**(2*n) + int(\n",
    "        2**(-0.5) * 2**(2**0.75*(n-1)) * (n>0) * (28**2 + 10)/config['hl_width'] #* (1-2**(-n))\n",
    "    )\n",
    "    #print(config['hl_width'])\n",
    "    #print(config['hl_depth'])\n",
    "    #print(num_params(config['hl_width'], config['hl_depth']))\n",
    "    print(config)\n",
    "    import ipdb; ipdb.set_trace()\n",
    "    main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
